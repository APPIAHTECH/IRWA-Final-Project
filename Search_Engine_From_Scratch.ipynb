{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HCTal9hwggsX"
   },
   "source": [
    "# Search-Engine-from-Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FwqvbsOAuKYq"
   },
   "source": [
    "# NAME(s) and NIA(s) \n",
    "\n",
    "**Author :** Stephen Appiah.\n",
    "         Umar Muhammad.\n",
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yVudfO1hzC4t"
   },
   "source": [
    "#### Load Python packages\n",
    "Let's first import all the packages that you will need during this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sEVugrgBzC4v",
    "outputId": "164d47c9-5cee-43f3-9ca7-846c525c1eca",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\work\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\work\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if you do not have 'nltk', the following command should work \"python -m pip install nltk\"\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "F9wOnbXuzC47",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from array import array\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "import numpy as np\n",
    "import collections\n",
    "import time\n",
    "from numpy import linalg as la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2srOPhiRzC48"
   },
   "source": [
    "#### Load data into memory\n",
    "Import the 500 Wikipedia articles (one article per line). For each article we have the document id, document title and document body separated by \"|\" character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "iKoCCw0qzC4_"
   },
   "outputs": [],
   "source": [
    "docs_path = 'inputs/documents-corpus.tsv'\n",
    "with open(docs_path) as fp:\n",
    "    lines = fp.readlines()\n",
    "lines = [l.strip().replace(' +', ' ') for l in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mii5yWGWzC5A",
    "outputId": "25bb1fc9-ef8d-4e44-d865-29aca0697880"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of Wikipedia articles in the corpus: 500\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of Wikipedia articles in the corpus: {}\".format(len(lines)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "94XWpWwUiTJF"
   },
   "source": [
    "# Building the Index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pBQKbz2Ki2F_"
   },
   "source": [
    "Processing the data\n",
    "\n",
    "Define functions to clean the text (tokenization / bigram/ lowercase /remove punctuation /stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "7Wt3c3wFl6rE"
   },
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    return [w for w in words if w.lower() not in stop_words]\n",
    "\n",
    "def stem_each(words):\n",
    "    return [stemmer.stem(w) for w in words]\n",
    "\n",
    "def remove_white_space(text):\n",
    "    return ' '.join(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "5eUBqpQeifVH"
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "  \"\"\"\n",
    "  Preprocess the article text (title + body) removing stop words, stemming,\n",
    "  transforming in lowercase and return the tokens of the text.\n",
    "\n",
    "  Argument:\n",
    "  line -- string (text) to be preprocessed\n",
    "\n",
    "  Returns:\n",
    "  line - a list of tokens corresponding to the input text after the preprocessing\n",
    "  \"\"\"\n",
    "  stemmer = PorterStemmer()\n",
    "  stop_words = set(stopwords.words(\"english\"))\n",
    "  text=  text.lower()\n",
    "  text = text.replace('#', '')\n",
    "  text=  text.split()\n",
    "  text= remove_stopwords(text)\n",
    "  text= stem_each(text)\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LxLqWJMnkCxo",
    "outputId": "582da5f7-f1e6-43b1-9c76-2471cb6353bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['smart-assist', 'like', 'apple’', 'siri', 'amazon’', 'alexa', 'recogn', 'pattern', 'speech', 'thank', 'voic', 'recognition,', 'infer', 'mean', 'provid', 'use', 'response.']\n"
     ]
    }
   ],
   "source": [
    "#Eample\n",
    "phrase = \"Smart-assistants like Apple’s Siri and Amazon’s Alexa recognize patterns in speech thanks to voice recognition, then infer meaning and provide a useful response.\"\n",
    "print(preprocess(phrase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Search Engine From Scratch",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
