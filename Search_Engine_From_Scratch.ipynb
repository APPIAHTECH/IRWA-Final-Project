{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HCTal9hwggsX"
   },
   "source": [
    "# Search-Engine-from-Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FwqvbsOAuKYq"
   },
   "source": [
    "# NAME(s) and NIA(s) \n",
    "\n",
    "**Author :** Stephen Appiah.\n",
    "         Umar Muhammad.\n",
    "**NIA :** 206637.\n",
    "      208244.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yVudfO1hzC4t"
   },
   "source": [
    "#### Load Python packages\n",
    "Let's first import all the packages that you will need during this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sEVugrgBzC4v",
    "outputId": "164d47c9-5cee-43f3-9ca7-846c525c1eca",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\work\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\work\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if you do not have 'nltk', the following command should work \"python -m pip install nltk\"\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "F9wOnbXuzC47",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from array import array\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "import numpy as np\n",
    "import collections\n",
    "import time\n",
    "from numpy import linalg as la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2srOPhiRzC48"
   },
   "source": [
    "#### Load data into memory\n",
    "Import the 500 Wikipedia articles (one article per line). For each article we have the document id, document title and document body separated by \"|\" character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "iKoCCw0qzC4_"
   },
   "outputs": [],
   "source": [
    "docs_path = 'inputs/documents-corpus.tsv'\n",
    "with open(docs_path) as fp:\n",
    "    lines = fp.readlines()\n",
    "lines = [l.strip().replace(' +', ' ') for l in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mii5yWGWzC5A",
    "outputId": "25bb1fc9-ef8d-4e44-d865-29aca0697880"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of Wikipedia articles in the corpus: 500\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of Wikipedia articles in the corpus: {}\".format(len(lines)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "94XWpWwUiTJF"
   },
   "source": [
    "# Building the Index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pBQKbz2Ki2F_"
   },
   "source": [
    "Processing the data\n",
    "\n",
    "Define functions to clean the text (tokenization / bigram/ lowercase /remove punctuation /stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "7Wt3c3wFl6rE"
   },
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    return [w for w in words if w.lower() not in stop_words]\n",
    "\n",
    "def stem_each(words):\n",
    "    return [stemmer.stem(w) for w in words]\n",
    "\n",
    "def remove_white_space(text):\n",
    "    return ' '.join(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "5eUBqpQeifVH"
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "  \"\"\"\n",
    "  Preprocess the article text (title + body) removing stop words, stemming,\n",
    "  transforming in lowercase and return the tokens of the text.\n",
    "\n",
    "  Argument:\n",
    "  line -- string (text) to be preprocessed\n",
    "\n",
    "  Returns:\n",
    "  line - a list of tokens corresponding to the input text after the preprocessing\n",
    "  \"\"\"\n",
    "  stemmer = PorterStemmer()\n",
    "  stop_words = set(stopwords.words(\"english\"))\n",
    "  text=  text.lower()\n",
    "  text = text.replace('#', '')\n",
    "  text=  text.split()\n",
    "  text= remove_stopwords(text)\n",
    "  text= stem_each(text)\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LxLqWJMnkCxo",
    "outputId": "582da5f7-f1e6-43b1-9c76-2471cb6353bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['smart-assist', 'like', 'apple’', 'siri', 'amazon’', 'alexa', 'recogn', 'pattern', 'speech', 'thank', 'voic', 'recognition,', 'infer', 'mean', 'provid', 'use', 'response.']\n"
     ]
    }
   ],
   "source": [
    "#Eample\n",
    "phrase = \"Smart-assistants like Apple’s Siri and Amazon’s Alexa recognize patterns in speech thanks to voice recognition, then infer meaning and provide a useful response.\"\n",
    "print(preprocess(phrase))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2srOPhiRzC48"
   },
   "source": [
    "#### Inverted Index Construction\n",
    "\n",
    "So the first step in building a text search engine is assembling an inverted index. An inverted index is a data structure that maps tokens to the documents they appear in. In this context, we can consider a token to simply be a word, so an inverted index, at its most basic, is just something that takes in a word, and returns to us a list of the documents it appears in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverted_index(lines, num_documents):\n",
    "    \"\"\"\n",
    "    Implement the inverted index and compute tf, df and idf\n",
    "    \n",
    "    Argument:\n",
    "    lines -- collection of Wikipedia articles\n",
    "    num_documents -- total number of documents\n",
    "    \n",
    "    Returns:\n",
    "    index - the inverted index (implemented through a Python dictionary) containing terms as keys and the corresponding\n",
    "    list of document these keys appears in (and the positions) as values.\n",
    "    tf - normalized term frequency for each term in each document\n",
    "    df - number of documents each term appear in\n",
    "    idf - inverse document frequency of each term\n",
    "    \"\"\"\n",
    "    index = defaultdict(list)\n",
    "    tf = defaultdict(list)\n",
    "    df = defaultdict(int)\n",
    "    title_index = defaultdict(str)\n",
    "    idf = defaultdict(float)\n",
    "    \n",
    "    for line in lines:\n",
    "        line_arr = line.split(\"|\")\n",
    "        page_id = int(line_arr[0])\n",
    "        terms = preprocess(''.join(line_arr[1:])) \n",
    "        title = line_arr[1]\n",
    "        title_index[page_id]=title\n",
    "        \n",
    "        ## As suggested in class, we are using a current_page_index ==> { ‘term1’: [current_doc, [list of positions]], ...,‘term_n’: [current_doc, [list of positions]]}\n",
    "        ## current_page_index ==> { ‘web’: [1, [0]], ‘retrieval’: [1, [1,4]], ‘information’: [1, [2]]}\n",
    "        current_page_index = {}\n",
    "        for position, term in enumerate(terms):\n",
    "            try:\n",
    "                current_page_index[term][page_id].append(position)\n",
    "            except:\n",
    "                current_page_index[term]=[page_id, array('I',[position])] #'I' indicates unsigned int (int in Python)\n",
    "            \n",
    "        # normalized the term frequencies\n",
    "        norm = 0\n",
    "        for term, posting in current_page_index.items():\n",
    "            norm += len(posting[1]) ** 2\n",
    "        norm = math.sqrt(norm)\n",
    "        \n",
    "        #calculate the tf(dividing the term frequency by the above computed norm) and df weights\n",
    "        for term, posting in current_page_index.items():\n",
    "            tf[term].append(np.round(len(posting[1])/norm,4))\n",
    "            df[term] = df[term] + 1 \n",
    "            \n",
    "        #merge the current page index with the main index\n",
    "        for term_page, posting_page in current_page_index.items():\n",
    "            index[term_page].append(posting_page)\n",
    "        \n",
    "        # Compute IDF\n",
    "        for term in df:\n",
    "            idf[term] = np.round(np.log(float(num_documents/df[term])), 4)\n",
    "            \n",
    "    return index, tf, df, idf, title_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_documents = len(lines)\n",
    "index, tf, df, idf, title_index = inverted_index(lines, num_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 Index results for the term 'research': \n",
      "[[33, array('I', [76])], [104, array('I', [633])], [131, array('I', [1257])], [139, array('I', [84])], [183, array('I', [1411])]]\n"
     ]
    }
   ],
   "source": [
    "#Eample\n",
    "print(\"First 5 Index results for the term 'research': \\n{}\".format(index['research'][:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "94XWpWwUiTJF"
   },
   "source": [
    "# Querying the Index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2srOPhiRzC48"
   },
   "source": [
    "#### Ranking\n",
    "\n",
    "When searching in a search engine, we are interested in obtain the results sorted by relevance or by some other criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_documents(terms, docs, index, idf, tf, title_index):\n",
    "    \"\"\"\n",
    "    Perform the ranking of the results of a search based on the tf-idf weights\n",
    "    \n",
    "    Argument:\n",
    "    terms -- list of query terms\n",
    "    docs -- list of documents, to rank, matching the query\n",
    "    index -- inverted index data structure\n",
    "    idf -- inverted document frequencies\n",
    "    tf -- term frequencies\n",
    "    title_index -- mapping between page id and page title\n",
    "    \n",
    "    Returns:\n",
    "    Print the list of ranked documents\n",
    "    \"\"\"\n",
    "    \n",
    "    doc_vectors = defaultdict(lambda: [0] * len(terms))\n",
    "    query_vector = [0] * len(terms)\n",
    "    query_terms_count = collections.Counter(terms)\n",
    "    query_norm = la.norm(list(query_terms_count.values()))\n",
    "    for termIndex, term in enumerate(terms):\n",
    "        if term not in index: continue\n",
    "        query_vector[termIndex]= query_terms_count[term]/query_norm * idf[term]\n",
    "        for doc_index, (doc, postings) in enumerate(index[term]):\n",
    "            if doc in docs:\n",
    "                doc_vectors[doc][termIndex] = tf[term][doc_index] * idf[term]  # TODO: check if multiply for idf\n",
    "    \n",
    "    doc_scores=[[np.dot(curDocVec, query_vector), doc] for doc, curDocVec in doc_vectors.items() ]\n",
    "    doc_scores.sort(reverse=True)\n",
    "    result_docs = [x[1] for x in doc_scores]\n",
    "    if len(result_docs) == 0: \n",
    "        print(\"No results found, try again\")\n",
    "        query = input()\n",
    "        docs = search(query, index)\n",
    "    return result_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2srOPhiRzC48"
   },
   "source": [
    "#### Search\n",
    "\n",
    "Enter queries to retrieved the top 10 relevant links based on the cosine similarity score (ltc.ltn schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, index):\n",
    "    \"\"\"\n",
    "    output is the list of documents that contain any of the query terms. \n",
    "    So, we will get the list of documents for each query term, and take the union of them.\n",
    "    \"\"\"\n",
    "    query = preprocess(query)\n",
    "    docs = set()\n",
    "    for term in query:\n",
    "        try:                      \n",
    "            term_docs=[posting[0] for posting in index[term]]\n",
    "            docs |= set(term_docs)\n",
    "        except:\n",
    "            pass\n",
    "    docs = list(docs)\n",
    "    ranked_docs = rank_documents(query, docs, index, idf, tf, title_index)\n",
    "    return ranked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert your query (i.e.: Computer Science):\n",
      "\n",
      "Computer Science\n",
      "\n",
      "======================\n",
      "Sample of 10 results out of 345 for the searched query:\n",
      "\n",
      "page_id= 2583 - page_title: Association of Synchronous Data Formats\n",
      "page_id= 297 - page_title: ACM Portal\n",
      "page_id= 2086 - page_title: Aperiodic finite state automaton\n",
      "page_id= 2784 - page_title: Australian Partnership for Advanced Computing\n",
      "page_id= 1220 - page_title: Aggregate function\n",
      "page_id= 1702 - page_title: American flag sort\n",
      "page_id= 647 - page_title: A Sharp   NET \n",
      "page_id= 956 - page_title: Adaptive Behavior\n",
      "page_id= 3939 - page_title: Bipolar violation\n",
      "page_id= 3190 - page_title: BTSharp\n"
     ]
    }
   ],
   "source": [
    "print(\"Insert your query (i.e.: Computer Science):\\n\")\n",
    "query = input()\n",
    "docs = search(query, index)\n",
    "top = 10\n",
    "\n",
    "print(\"\\n======================\\nSample of {} results out of {} for the searched query:\\n\".format(top, len(docs)))\n",
    "for d_id in docs[:top]:\n",
    "    print(\"page_id= {} - page_title: {}\".format(d_id, title_index[d_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Search Engine From Scratch",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
